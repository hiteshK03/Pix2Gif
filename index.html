
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="diffusion creates Gifs from images using motion-magnitude guidance.">
  <meta name="keywords" content="Pix2Gif, image-to-video, diffusion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Pix2Gif: Motion-Guided Diffusion for Gif Generation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/grid.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/bibtex-js/0.3.0/bibtex.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    .video-margin {
      margin-bottom: 20px; 
    }
  </style>
  <style>
  .reduce-space {
    margin-bottom: -100px;  
  }
</style>
</head>
<body>


<section class="hero">
    <div class="reduce-space">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                <h1 class="title is-1 publication-title"><span class='main-title'>Pix2Gif: Motion-Guided Diffusion <br> for GIF Generation</span></h1>
                <div class="is-size-5 publication-authors">
                    <span class="author-block">
                        <a href="https://hiteshk03.github.io/">Hitesh Kandala</a><sup>1</sup>,
                    </span>
                    <span class="author-block">
                      <a href="https://www.microsoft.com/en-us/research/people/jfgao/">Jianfeng Gao</a><sup>2</sup>,
                    </span>                    
                    <span class="author-block">
                        <a href="https://jwyang.github.io/">Jianwei Yang</a><sup>2</sup>
                    </span>
                    </div>
        
                    <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1*</sup>AMD,</span>
                    <span class="author-block"><sup>2</sup>Microsoft Research Redmond</span>
                    </div>  

                    <div class="is-size-7 publication-authors">
                      <span class="author-block"><sup>*</sup>work done while at Microsoft Research</span>
                      </div>  
  
                    <div class="column has-text-centered">
                        <div class="publication-links">
                        <!-- PDF Link. -->
                        <span class="link-block">
                          <a href="https://arxiv.org/pdf/2403.04634.pdf"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="fas fa-file-pdf"></i>
                          </span>
                          <span>Paper</span>
                          </a>
                      </span>
                      <span class="link-block">
                          <a href="https://arxiv.org/abs/2403.04634"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="ai ai-arxiv"></i>
                          </span>
                          <span>arXiv</span>
                          </a>
                      </span>
                      <!-- Demo Link. -->
                      <!-- <span class="link-block">
                          <a href="https://520a83a7524ec7d864.gradio.live"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="fab fa-hotjar"></i>
                          </span>
                          <span>Demo</span>
                          </a>
                      </span> -->
                      <!-- Code Link. -->
                        <span class="link-block">
                            <a href="https://github.com/hiteshK03/Pix2Gif"
                            class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                            </a>
                        </span>
                        <!-- Dataset Link. -->
                        <!-- <span class="link-block">
                            <a href="https://github.com/google/nerfies/releases/tag/0.1"
                            class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                                <i class="far fa-images"></i>
                            </span>
                            <span>Data</span>
                            </a> -->
                        </div>
                    </div>
                </div>
                </div>
            </div>
        </div>
    </div>
    </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <!-- <div class="rows is-centered">
        <figure>
          <iframe width="100%" height="56.25%" src="https://www.youtube.com/embed/3vo6mzk9K4s?si=cLkofep9H7AHOm5V?&playlist=3vo6mzk9K4s&loop=1&autoplay=1&mute=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

          <figcaption class="tagline">
            Our one-step generator achieves comparable image quality with StableDiffusion v1.5 while being 30x faster.
        </figure>
        <div class="content has-text-justified">
          <p>
            Diffusion models are known to approximate the score function of the distribution they are trained on. 
            In other words, an unrealistic synthetic image can be directed toward higher probability density region through the denoising process (see <a href="https://dreamfusion3d.github.io" style="color: red;">SDS</a>). 
            Our core idea is training two diffusion models to estimate not only the score function of the target real distribution, but also that of the fake distribution. 
            We construct a gradient update to our generator as the difference between the two scores, essentially nudging the generated images toward higher realism as well as lower fakeness (see <a href="https://ml.cs.tsinghua.edu.cn/prolificdreamer/" style="color: red;">VSD</a>). 
            Our method is similar to GANs in that a critic is jointly trained with the generator to minimize a divergence between the real and fake distributions, but differs in that our training does not play an adversarial game that may cause training instability, and our critic can fully leverage the weights of a pretrained diffusion model. 
            Combined with a simple regression loss to match the output of the multi-step diffusion model, our method outperforms all published few-step diffusion approaches, reaching 2.62 FID on ImageNet 64x64 and 11.49 FID on zero-shot COCO-30k, comparable to Stable Diffusion but orders of magnitude faster. 
            Utilizing FP16 inference, our model generates images at 20 FPS on modern hardware.
        </p>
        </div>
        <br> 
      </div> -->
    <!--/ Abstract. -->
  </div>
  <!-- <div class="container is-max-desktop">
    <div class="rows is-centered">
      <div class="row">
        <h2 class="title is-3 has-text-centered">DMD Method Overview</h2>
      </div>
      <br><br>
      <img src="images/overview.png" alt="Method Overview">
      <br><br>
      <p>
        We train one-step generator <strong>G<sub>θ</sub></strong> to map random noise <strong>z</strong> into a realistic image. 
        To match the multi-step sampling outputs of the diffusion model, we pre-compute a collection of noise--image pairs, and occasionally load the noise from the collection and enforce LPIPS 
        <span style="color: green;">regression loss</span> between our one-step generator and the diffusion output. 
        Furthermore, we provide <strong><span style="color: red;">distribution matching gradient ∇<sub>θ</sub> D<sub>KL</sub></span></strong> to the fake image to enhance realism. 
        We inject a random amount of noise to the fake image and pass it to two diffusion models, one pretrained on the real data and the other continually trained on the fake images with a 
        <span style="color: blue;">diffusion loss</span>, to obtain its denoised versions. The denoising scores (visualized as mean prediction in the plot) indicate directions to make the images more realistic or fake. The difference between the two represents the direction toward more realism and less fakeness and is backpropagated to the one-step generator.
      </p>      
    </div>
  </div> -->
  <br><br>
  <div class="container is-max-desktop">
  <div class="rows is-centered">
  <div class="grid">
    <div class="grid-item">
      <p class="image-label"><strong>Prompt: The man is riding a horse.</strong></p>
      <img src="static/imgs/west_world.gif" alt="Pix2Gif">
      <p class="image-label">Source: Westworld/HBO</p>
    </div>
    <div class="grid-item">
      <p class="image-label"><strong>Prompt: The Joker is talking and smiling.</strong></p>
      <img src="static/imgs/batman_joker.gif" alt="Pix2Gif">
      <p class="image-label">Source: Joker/Batman:Dark Knight</p>
    </div>
    <div class="grid-item">
      <p class="image-label"><strong>Prompt: A big wave.</strong></p>
      <img src="static/imgs/japan_wave_pix2gif.gif" alt="Pix2Gif">
      <p class="image-label">Source: The Great Wave off Kanagawa</p>
    </div>
  </div>
  <div class="rows is-centered">
    <div class="grid">
      <div class="grid-item">
        <p class="image-label"><strong>Prompt: A big sea wave.</strong></p>
        <img src="static/imgs/big_sea_wave.gif" alt="Pix2Gif">
        <p class="image-label">Source: Brett Allen/Shutterstock.com</p>
      </div>   
      <div class="grid-item">
        <p class="image-label"><strong>Prompt: Two person are walking.</strong></p>
        <img src="static/imgs/two_person_walking_pix2gif.gif" alt="Pix2Gif">
        <p class="image-label">Source: Malte Mueller/Getty Images</p>
      </div>
      <div class="grid-item">
        <p class="image-label"><strong>Prompt: The horse is walking.</strong></p>
        <img src="static/imgs/horse_is_walking.gif" alt="Pix2Gif">
        <p class="image-label">Source: Ernie Cowan</p>
      </div>
    </div>
  </div>
  </div>
  </div>
  <br><br>
  <div class="container is-max-desktop">
    <div class="rows is-centered">
      <div class="row">
        <h2 class="title is-3 has-text-centered">Abstract</h2>
      </div>
      <br><br>
      <p>
        We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video) generation. We tackle this problem differently by formulating the task as an image translation problem steered by text and motion magnitude prompts. 
        To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source image conditioned on the two types of prompts. 
        Furthermore, we introduce a perceptual loss to ensure the transformed feature map remains within the same space as the target image, ensuring content consistency and coherence. 
        In preparation for the model training, we meticulously curated data by extracting coherent image frames from the TGIF video-caption dataset, which provides rich information about the temporal changes of subjects. 
        After pretraining, we apply our model in a zero-shot manner to a number of video datasets. Extensive qualitative and quantitative experiments demonstrate the effectiveness of our model -- it not only captures the semantic prompt from text but also the spatial ones from motion guidance. 
        We train all our models using a single node of 16xV100 GPUs.        
      </p>    
      <br><br>
      <br><br>

      <div class="row">
        <h2 class="title is-3 has-text-centered">Pix2Gif</h2>
      </div>
      <br><br>
      <p>
        Our model is built on the Stable Diffusion but with newly introduced motion-guided warping module. We formualte the GIF generation as a temporal instructed image editing problem. 
      </p>
      <br><br>
      <img src="images/model_upscale.png" alt="Method Overview">
      <br><br>
      <br><br>

      <div class="row">
        <h2 class="title is-3 has-text-centered">Our Examples</h2>
      </div>

      <br><br>
      
      <div class="comparison-set">
        <p style="text-align: center; font-size: 20px;">Prompt: A cat is playing with wool.</p>
        <div class="grid">
          <div class="grid-item">
            <img src="static/imgs/cat_playing_wool.jpeg" alt="Input Image">
            <p class="image-label"><strong>Input Image</strong></p>
          </div>
          <div class="grid-item">
            <img src="static/imgs/cat_playing_wool_pix2gif.gif" alt="Pix2Gif Image">
            <p class="image-label"><strong>Ours (Pix2Gif)</strong></p>
          </div>
        </div>
      </div>

      <br><br>

      <div class="row">
        <h2 class="title is-3 has-text-centered">Compositionality of actions</h2>
      </div>

      <br><br>
      
      <div class="comparison-set">
        <!-- <p style="text-align: center; font-size: 20px;">Prompt: A cat is playing with wool.</p> -->
        <div class="grid">
          <div class="grid-item">
            <img src="static/imgs/cat_is_playing_with_wool.gif" alt="Input Image">
            <p class="image-label">Action 1: cat is playing with wool.</p>
          </div>
          <div class="grid-item">
            <img src="static/imgs/cat_is_dancing.gif" alt="Pix2Gif Image">
            <p class="image-label">Action 2: cat is dancing.</p>
          </div>
          <div class="grid-item">
            <img src="static/imgs/cat_is_dancing_while_playing_with_wool.gif" alt="Pix2Gif Image">
            <p class="image-label">Action 1+2: cat is dancing while playing with wool.</p>
          </div>          
        </div>
      </div>

      <br><br>

      <div class="row">
        <h2 class="title is-3 has-text-centered">Comparison to state-of-the-art Image-to-Video Methods</h2>
      </div>

      <br><br>

      <div class="comparison-set">
        <p style="text-align: center; font-size: 20px;">Prompt: A cat is playing with wool.</p>
        <div class="grid">
          <div class="grid-item">
            <img src="static/imgs/cat_playing_wool.jpeg" alt="Input Image">
            <p class="image-label"><strong>Input Image</strong></p>
          </div>
          <div class="grid-item">
            <img src="static/imgs/cat_dynamic_crafter.gif" alt="DynamiCrafter Image">
            <p class="image-label"><strong><a href="https://huggingface.co/spaces/Doubiiu/DynamiCrafter">DynamiCrafter</a></strong></p>
          </div>
          <div class="grid-item">
            <img src="static/imgs/A_cat_is_playing_with_wool._seed870430723537518.gif" alt="Pika Labs Image">
            <p class="image-label"><strong><a href="https://pika.art/">Pika Labs</a></strong></p>
          </div>
          <div class="grid-item">
            <img src="static/imgs/cat_playing_wool_pix2gif.gif" alt="Pix2Gif Image">
            <p class="image-label"><strong>Ours</strong></p>
          </div>
        </div>
        <br>
        <p style="text-align: center; font-size: 20px;">Prompt: The two person are running.</p>
        <div class="grid">
          <div class="grid-item">
            <img src="static/imgs/two_person_walking.jpg" alt="Input Image">
            <p class="image-label"><strong>Input Image</strong></p>
          </div>
          <div class="grid-item">
            <img src="static/imgs/two_person_walking_dynamicrafter.gif" alt="DynamiCrafter Image">
            <p class="image-label"><strong><a href="https://huggingface.co/spaces/Doubiiu/DynamiCrafter">DynamiCrafter</a></strong></p>
          </div>
          <div class="grid-item">
            <img src="static/imgs/two_person_walking_pika.gif" alt="Pika Labs Image">
            <p class="image-label"><strong><a href="https://pika.art/">Pika Labs</a></strong></p>
          </div>
          <div class="grid-item">
            <img src="static/imgs/two_person_walking_pix2gif.gif" alt="Pix2Gif Image" height="300">
            <p class="image-label"><strong>Ours</strong></p>
          </div>
        </div>
        <br>
        <p style="text-align: center; font-size: 20px;">Prompt: A big wave.</p>
        <div class="grid">
          <div class="grid-item">
            <img src="static/imgs/japan_wave.jpg" alt="Input Image">
            <p class="image-label"><strong>Input Image</strong></p>
          </div>
          <div class="grid-item">
            <img src="static/imgs/japan_wave_dynamicrafter.gif" alt="DynamiCrafter Image">
            <p class="image-label"><strong><a href="https://huggingface.co/spaces/Doubiiu/DynamiCrafter">DynamiCrafter</a></strong></p>
          </div>
          <div class="grid-item">
            <img src="static/imgs/japan_wave_pika.gif" alt="Pika Labs Image">
            <p class="image-label"><strong><a href="https://pika.art/">Pika Labs</a></strong></p>
          </div>
          <div class="grid-item">
            <img src="static/imgs/japan_wave_pix2gif.gif" alt="Pix2Gif Image" height="300">
            <p class="image-label"><strong>Ours</strong></p>
          </div>
        </div>
        <br>
        <p style="text-align: center; font-size: 20px;">Prompt: The wind is blowing the flower.</p>
        <div class="grid">
          <div class="grid-item">
            <img src="static/imgs/rose.jpg" alt="Input Image">
            <p class="image-label"><strong>Input Image</strong></p>
          </div>
          <div class="grid-item">
            <img src="static/imgs/wind_blow_rose_dynamicrafter.gif" alt="DynamiCrafter Image">
            <p class="image-label"><strong><a href="https://huggingface.co/spaces/Doubiiu/DynamiCrafter">DynamiCrafter</a></strong></p>
          </div>
          <div class="grid-item">
            <img src="static/imgs/wind_blow_rose_pika.gif" alt="Pika Labs Image">
            <p class="image-label"><strong><a href="https://pika.art/">Pika Labs</a></strong></p>
          </div>
          <div class="grid-item">
            <img src="static/imgs/wind_blow_rose.gif" alt="Pix2Gif Image" height="300">
            <p class="image-label"><strong>Ours</strong></p>
          </div>
        </div>        
      </div>
      <br>    
      <br><br>      
      <div class="row">
        <h2 class="title is-3 has-text-centered">Dataset</h2>
      </div>
      <br><br>
      <p>
        We use the <a href="https://github.com/raingo/TGIF-Release">TGIF dataset</a> for our model training. The dataset contains 100K animated GIFs with captions. We extract frames from the GIFs and use the captions as the text prompts. We further curate the dataset by removing the GIFs with less than 5 frames and the GIFs with the same captions. The final dataset contains 100K GIFs with 5-20 frames. We split the dataset into 80K for training and 20K for testing. 
      </p>
      <br><br>
      <img src="images/dataset_frames.png" alt="Frames">
      <br><br> 
      <div class="comparison-set">
        <div class="grid">
          <div class="grid-data">
            <img src="images/dataset_filter.png" alt="Filter">
          </div>
          <div class="grid-data">
            <img src="images/dataset_final.png" alt="Final">
          </div>
        </div>
      </div>
      <br><br>
    </div>
  </div>
</section>
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{kandala2024pix2gif,
      title={Pix2Gif: Motion-Guided Diffusion for GIF Generation}, 
      author={Hitesh Kandala and Jianfeng Gao and Jianwei Yang},
      year={2024},
      eprint={2403.04634},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
  </div>
</section>


<footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content has-text-centered">
            <p>
              Website source based on <a href="https://github.com/nerfies/nerfies.github.io">this source code</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>